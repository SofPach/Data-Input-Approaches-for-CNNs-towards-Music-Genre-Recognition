{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5c09ff-9a1a-484e-b17a-40a4a2c78af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam, Adadelta\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils  ## .py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f22add-bef1-484a-9315-1575b56ce703",
   "metadata": {},
   "source": [
    "### Load meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd8af6-fd88-4a09-8f1c-fb591baabff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where mp3 are stored.  ### in terminal write: export AUDIO_DIR=./'directory where the mp3 are stored'\n",
    "AUDIO_DIR=os.environ.get('AUDIO_DIR')\n",
    "\n",
    "# Load metadata and features.\n",
    "tracks_ = utils.load('fma/data/fma_metadata/tracks.csv')\n",
    "genres = utils.load('fma/data/fma_metadata/genres.csv')\n",
    "features = utils.load('fma/data/fma_metadata/features.csv')\n",
    "echonest = utils.load('fma/data/fma_metadata/echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks_.index)\n",
    "assert echonest.index.isin(tracks_.index).all()\n",
    "\n",
    "tracks_.shape, genres.shape, features.shape, echonest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ee858-041c-4bc9-8e27-e46e5c397cdc",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa75045-fbea-4ca3-b1c0-b97a29d9ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Possible trainings\n",
    "model_dim = '2d' ## '1d' or '2d' 'both'\n",
    "Division = True ## if data gets diveded in small pieces\n",
    "Drop_out = False\n",
    "\n",
    "N = 7994 ## number of data samples we will work with (max = 7994)\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "lr = 5e-4\n",
    "\n",
    "model_saving_name = 'model.pt'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1330bd-6278-492b-80f0-e8c63dc13049",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1_000_000  ## max = 1_321_438\n",
    "n_fft = 2048  ## for the STFT fourier transform\n",
    "n_rows = int(np.round((n_fft+1.1)/2))\n",
    "hop_length = n_fft//2  ## n_fft/a  1/a = overlap\n",
    "n_columns = int(np.ceil(max_len/hop_length))\n",
    "if Division == True:\n",
    "    num_divisions = 7\n",
    "    stride_calc = 919 \n",
    " #int(max_len/(N+19))-200\n",
    "\n",
    "elif Division == False:\n",
    "    num_divisions = 1\n",
    "    stride_calc = 1007\n",
    "\n",
    "if Drop_out == True:\n",
    "    drop_out = 0.2\n",
    "elif Drop_out == False:\n",
    "    drop_out = 0\n",
    "\n",
    "\n",
    "n_columns_division = int(n_columns/num_divisions)\n",
    "n_data_division = int(max_len/num_divisions)\n",
    "\n",
    "print(f'Shape of data 1D: (1,{n_data_division})')\n",
    "print(f'Sape of data 2D: {n_rows,n_columns_division}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0186bc-87aa-49de-ad60-886d137568f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data \"preprocessing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a23d6-03cf-4ee2-89e7-5cf194bc1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUBSETING , we are only going to work with the small data_set and oly with N samples.\n",
    "\n",
    "subset = tracks_.index[tracks_['set', 'subset'] <= 'small'].drop([98565,98567,98569,99134,108925,133297])\n",
    "tracks = tracks_.loc[subset].iloc[:N]\n",
    "\n",
    "## taking the different indexes for train, validation and testing\n",
    "train = tracks.index[tracks['set', 'split'] == 'training'].values\n",
    "val = tracks.index[tracks['set', 'split'] == 'validation'].values\n",
    "test = tracks.index[tracks['set', 'split'] == 'test'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76813b10-e927-4bd9-b284-aad7364e145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the \"Y\" vectors (classification of the data) as onehot encoded\n",
    "labels_onehot = LabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n",
    "labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)\n",
    "number_genre = labels_onehot.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa67ca4-2008-47b1-ad1d-f4e7cd04ad3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a06786-6321-4326-a1c4-d363314d72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTLoader(utils.Loader):\n",
    "    raw_loader = utils.LibrosaLoader()\n",
    "\n",
    "    def load(self, filename):\n",
    "        import librosa\n",
    "        x = self.raw_loader.load(filename)\n",
    "        x_short = x[:max_len] ## getting only up to a certain length of the data sample (to have the same length for all data)\n",
    "        \n",
    "        stft = np.abs(librosa.stft(x_short, n_fft=n_fft, hop_length=hop_length))  ## n_fft is the size of the FFT,, hop_length is frame increment between \n",
    "        ## two consecutive FFT\n",
    "        return stft ,x# return a matrix (q,p) where p is the number of columns of the matrix (important afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea1e12-2a6e-45a4-af92-55f5a06f2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_1d = utils.LibrosaLoader(sampling_rate=44100) ## 1d loader for raw audio\n",
    "loader_2d = STFTLoader()  # 2d loader for stft matrix\n",
    "loader_2d.load('fma/data/fma_small/000/000002.mp3')[1].shape  ##example for 2D data shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082377eb-8c34-45ae-b767-9061bd82d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_1d(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, tids, onehot): ## tids = train, val or test. Are the indixes\n",
    "        'Initialization'\n",
    "        self.labels = onehot\n",
    "        self.tids = tids\n",
    "        self.loader = loader_1d\n",
    "        self.audio_path = utils.get_audio_path\n",
    "        self.audio_dir = AUDIO_DIR\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.tids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        y = self.labels.loc[self.tids[index]]\n",
    "        audio_name = self.audio_path(self.audio_dir, self.tids[index])\n",
    "        x = self.loader.load(audio_name)[:max_len]\n",
    "        \n",
    "        X_ = torch.from_numpy(np.array(x,dtype='float32'))\n",
    "        X = X_.reshape( *X_.shape,1)\n",
    "        Y = torch.from_numpy(np.array(y,dtype='float32'))\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fa39a-091f-4cf5-8465-6f8c570b182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_2d(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, tids, onehot): ## tids = train, val or test. Are the indixes\n",
    "        'Initialization'\n",
    "        self.labels = onehot\n",
    "        self.tids = tids\n",
    "        self.loader = loader_2d\n",
    "        self.audio_path = utils.get_audio_path\n",
    "        self.audio_dir = AUDIO_DIR\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.tids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        y = self.labels.loc[self.tids[index]]\n",
    "        audio_name = self.audio_path(self.audio_dir, self.tids[index])\n",
    "        x = self.loader.load(audio_name)[0]\n",
    "        \n",
    "        X = torch.from_numpy(np.array(x,dtype='float32')).permute(1,0)\n",
    "        Y = torch.from_numpy(np.array(y,dtype='float32'))\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757342a-1264-4b9e-afbc-add974ebd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_both(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, tids, onehot): ## tids = train, val or test. Are the indixes\n",
    "        'Initialization'\n",
    "        self.labels = onehot\n",
    "        self.tids = tids\n",
    "        self.loader = loader_2d\n",
    "        self.audio_path = utils.get_audio_path\n",
    "        self.audio_dir = AUDIO_DIR\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.tids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        y = self.labels.loc[self.tids[index]]\n",
    "        audio_name = self.audio_path(self.audio_dir, self.tids[index])\n",
    "        x_2, x_1  = self.loader.load(audio_name)\n",
    "        \n",
    "        X_1_ = torch.from_numpy(np.array(x_1,dtype='float32'))\n",
    "        X_2 = torch.from_numpy(np.array(x_2,dtype='float32')).permute(1,0)\n",
    "        X_1 = X_1_.reshape(*X_1_.shape ,1)[:max_len]\n",
    "        \n",
    "        Y = torch.from_numpy(np.array(y,dtype='float32'))\n",
    "\n",
    "        return X_1, X_2, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504255c-8f88-46f5-944b-fbd7d70e137e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d71359-da58-45ab-8a44-50df089556dd",
   "metadata": {},
   "source": [
    "#### 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ce2d2-410d-44a2-ae09-6d42829a3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network1d(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_columns, kernel_size): ## kernell size has to be [a, n_rows] \n",
    "        super().__init__()\n",
    "        self.conv_path_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=256, kernel_size = [stride_calc*20, 1], stride = [stride_calc, 1], padding=[0, 0]), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.conv_path_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[2,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[1,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=[n_columns-kernel_size[0]+1,1]) ## -(kernel_size[0]-1) since is the dim of conv path 1\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=[n_columns-kernel_size[0]+1,1])\n",
    "        \n",
    "        self.linear_path = nn.Sequential(\n",
    "            nn.Linear(in_features=256*2, out_features=300), ## in is n_fft*2 (2 different poolings)\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=300, out_features=150),\n",
    "            nn.BatchNorm1d(150),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=150, out_features=number_genre), ## out number of genres\n",
    "            nn.BatchNorm1d(number_genre),\n",
    "            nn.ELU()\n",
    "        )\n",
    "               \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1_1 = self.conv_path_1(x)\n",
    "        x1_2 = self.conv_path_2(x1_1)\n",
    "        x2 = x1_1 + x1_2\n",
    "        x3 = torch.cat((self.maxpool(x2),self.avgpool(x2)),2)\n",
    "        x4 = self.linear_path(self.flatten(x3))\n",
    "\n",
    "        \n",
    "        return x4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6e22b-e2c2-4944-95a7-884d154decbc",
   "metadata": {},
   "source": [
    "#### 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fd461-a46d-4f66-aa52-2ec1b3578f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_columns, kernel_size): ## kernell size has to be [a, n_rows] \n",
    "        super().__init__()\n",
    "        self.conv_path_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=256, kernel_size=kernel_size,padding=[0,0]), ## does a convolution over the width so yileds a 1d vector\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.conv_path_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[2,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[1,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=[n_columns-kernel_size[0]+1,1]) ## -(kernel_size[0]-1) since is the dim of conv path 1\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=[n_columns-kernel_size[0]+1,1])\n",
    "        \n",
    "        self.linear_path = nn.Sequential(\n",
    "            nn.Linear(in_features=256*2, out_features=300), ## in is n_fft*2 (2 different poolings)\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=300, out_features=150),\n",
    "            nn.BatchNorm1d(150),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=150, out_features=number_genre), ## out number of genres\n",
    "            nn.BatchNorm1d(number_genre),\n",
    "            nn.ELU()\n",
    "        )\n",
    "               \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1_1 = self.conv_path_1(x)\n",
    "        x1_2 = self.conv_path_2(x1_1)\n",
    "        x2 = x1_1 + x1_2\n",
    "        x3 = torch.cat((self.maxpool(x2),self.avgpool(x2)),2)\n",
    "        x4 = self.linear_path(self.flatten(x3))\n",
    "\n",
    "        \n",
    "        return x4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f606174-38b9-45de-8247-8726bfc419aa",
   "metadata": {},
   "source": [
    "#### Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db5d39-655b-42c4-a185-c42582e709ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkBoth(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_columns, kernel_size): ## kernell size has to be [a, n_rows] \n",
    "        super().__init__()\n",
    "        self.conv_path_1_2d = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=128, kernel_size=kernel_size,padding=[0,0]), ## does a convolution over the width so yileds a 1d vector\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.conv_path_1_1d = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=128, kernel_size = [stride_calc*20, 1], stride = [stride_calc, 1], padding=[0, 0]), ## convolution over the 1d input to match sizes\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.conv_path_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[2,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[1,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=[n_columns-kernel_size[0]+1,1]) ## -(kernel_size[0]-1) since is the dim of conv path 1\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=[n_columns-kernel_size[0]+1,1])\n",
    "        \n",
    "        self.linear_path = nn.Sequential(\n",
    "            nn.Linear(in_features=256*2, out_features=300), ## in is n_fft*2 (2 different poolings)\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=300, out_features=150),\n",
    "            nn.BatchNorm1d(150),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=150, out_features=number_genre), ## out number of genres\n",
    "            nn.BatchNorm1d(number_genre),\n",
    "            nn.ELU()\n",
    "        )\n",
    "               \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x_1d, x_2d):\n",
    "        \n",
    "        x1_1d = self.conv_path_1_1d(x_1d)\n",
    "        x1_2d = self.conv_path_1_2d(x_2d)\n",
    "        x1_together = torch.cat((x1_1d, x1_2d), 1)\n",
    "        \n",
    "        x2 = self.conv_path_2(x1_together)\n",
    "        x3 = x1_together + x2\n",
    "        x4 = torch.cat((self.maxpool(x3),self.avgpool(x3)),2)\n",
    "        x5 = self.linear_path(self.flatten(x4))\n",
    "        \n",
    "        return x5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e730c8-02b1-48ed-89f0-1680b090f35e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e068a43-df4e-4df4-9893-350aea2d4541",
   "metadata": {},
   "source": [
    "#### 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c178ac57-d4d5-486e-9289-b5cd37e4a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training for model 1d\n",
    "\n",
    "if model_dim == '1d':\n",
    "    model = Network1d(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                                                                                \n",
    "    train_data = Dataset_1d(train, labels_onehot)\n",
    "    valid_data = Dataset_1d(val, labels_onehot)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "\n",
    "    opt = Adam(model.parameters(), lr = lr)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    best_val = np.inf\n",
    "    \n",
    "    loss_train = np.array([])\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_avg = 0\n",
    "        loss_avg_val = 0\n",
    "        n_loss = 0\n",
    "        n_loss_val = 0\n",
    "        \n",
    "        model.train()\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        iterator = tqdm(train_loader)\n",
    "        for batch_x, batch_y in (iterator):\n",
    "            \n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "            \n",
    "            y_pred = 0\n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                batch_x_j = batch_x[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                \n",
    "                y_pred_j = model(batch_x_j) \n",
    "                loss = loss_fn(y_pred_j,batch_y) ## the loss is computed and the gradient is computed inside the for loop, for each segment\n",
    "                n_loss = n_loss+1\n",
    "\n",
    "                loss_avg = loss_avg + loss.detach().cpu().numpy()\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                iterator.set_description(f\"Train loss: {loss.detach().cpu().numpy()}\")\n",
    "    \n",
    "            \n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            predictions = []\n",
    "            true = []\n",
    "            for batch_x, batch_y in tqdm(valid_loader):\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "    \n",
    "                batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "                \n",
    "    \n",
    "                y_pred = 0\n",
    "                for j in range(max([num_divisions*2-3,1])):\n",
    "                    \n",
    "                    batch_x_j = batch_x[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                    \n",
    "                    y_pred_j = model(batch_x_j)\n",
    "                    \n",
    "                    loss_avg_val = loss_avg_val + loss_fn(y_pred_j, batch_y).detach().cpu().numpy() ## this loss (inside) is to compare with training loss, is computed for each segment\n",
    "                    n_loss_val = n_loss_val + 1\n",
    "                    \n",
    "                    y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "                \n",
    "                predictions.append(y_pred) \n",
    "                true.append(batch_y)\n",
    "            predictions = torch.cat(predictions, axis=0)\n",
    "            true = torch.cat(true, axis=0)   ## the loss is computed OUTSIDE the for loop, once the predictions for the whole song is computed\n",
    "                                            ## this is the loss that we use to decide to save the model or not.\n",
    "            val_loss = loss_fn(predictions, true)\n",
    "    \n",
    "            index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "            zeros = np.zeros(predictions.shape)\n",
    "            zeros[np.arange(index.shape[0]), index]=1\n",
    "            \n",
    "            val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "                \n",
    "            print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
    "        loss_val_epoch.append(loss_avg_val/n_loss_val)\n",
    "        loss_train_epoch.append(loss_avg/n_loss)\n",
    "        \n",
    "        if val_loss < best_val:\n",
    "            print(\"Saved Model\")\n",
    "            torch.save(model.state_dict(), model_saving_name)\n",
    "            best_val = val_loss\n",
    "    \n",
    "    plt.plot(np.arange(1,epoch+2),loss_train_epoch,label='Training Loss')\n",
    "    plt.plot(np.arange(1,epoch+2),loss_val_epoch,label='Validation Loss (\"segment-wise\")')\n",
    "    plt.legend()\n",
    "\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28884fad-0656-4d15-9675-abbd0d8d9e33",
   "metadata": {},
   "source": [
    "#### 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a93e8-03ac-4b85-84e3-366770e705db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training for model 2d\n",
    "\n",
    "if model_dim == '2d':\n",
    "    model = Network2d(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "    \n",
    "    train_data = Dataset_2d(train, labels_onehot)\n",
    "    valid_data = Dataset_2d(val, labels_onehot)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "\n",
    "    opt = Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    best_val = np.inf\n",
    "    \n",
    "    loss_train = np.array([])\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    \n",
    "    for epoch in range(epochs):      \n",
    "        loss_avg = 0\n",
    "        loss_avg_val = 0\n",
    "        n_loss = 0\n",
    "        n_loss_val = 0\n",
    "        \n",
    "        model.train()\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        iterator = tqdm(train_loader)\n",
    "        for batch_x, batch_y in (iterator):\n",
    "            \n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "            \n",
    "            y_pred = 0\n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                batch_x_j = batch_x[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "\n",
    "                y_pred_j = model(batch_x_j)\n",
    "                loss = loss_fn(y_pred_j,batch_y)\n",
    "\n",
    "                n_loss = n_loss+1\n",
    "                loss_avg = loss_avg + loss.detach().cpu().numpy()\n",
    "    \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                iterator.set_description(f\"Train loss: {loss.detach().cpu().numpy()}\")\n",
    "                \n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            predictions = []\n",
    "            true = []\n",
    "            for batch_x, batch_y in tqdm(valid_loader):\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "    \n",
    "                batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "                \n",
    "    \n",
    "                y_pred = 0\n",
    "                for j in range(max([num_divisions*2-3,1])):\n",
    "                    \n",
    "                    batch_x_j = batch_x[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                    \n",
    "                    y_pred_j = model(batch_x_j)\n",
    "                    y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "                    \n",
    "                    loss_avg_val = loss_avg_val + loss_fn(y_pred_j, batch_y).detach().cpu().numpy()\n",
    "                    n_loss_val = n_loss_val + 1\n",
    "                \n",
    "                predictions.append(y_pred)\n",
    "                true.append(batch_y)\n",
    "            predictions = torch.cat(predictions, axis=0)\n",
    "            true = torch.cat(true, axis=0)\n",
    "            val_loss = loss_fn(predictions, true)\n",
    "    \n",
    "            index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "            zeros = np.zeros(predictions.shape)\n",
    "            zeros[np.arange(index.shape[0]), index]=1\n",
    "            \n",
    "            val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "                \n",
    "            print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
    "        loss_val_epoch.append(loss_avg_val/n_loss_val)\n",
    "        loss_train_epoch.append(loss_avg/n_loss)\n",
    "        \n",
    "        if val_loss < best_val:\n",
    "            print(\"Saved Model\")\n",
    "            torch.save(model.state_dict(), model_saving_name)\n",
    "            best_val = val_loss\n",
    "    plt.plot(np.arange(1,epoch+2),loss_train_epoch,label='Training Loss')\n",
    "    plt.plot(np.arange(1,epoch+2),loss_val_epoch,label='Validation Loss (\"segment-wise\")')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd4fe7-c49a-49fa-860c-3c91570adfc7",
   "metadata": {},
   "source": [
    "#### Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aded63-b616-4e75-ad20-9c0fa0fda2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_dim == 'both':\n",
    "    model = NetworkBoth(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                \n",
    "    train_data = Dataset_both(train, labels_onehot)\n",
    "    valid_data = Dataset_both(val, labels_onehot)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "\n",
    "    opt = Adam(model.parameters(),lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "  \n",
    "    model.to(device)\n",
    "    \n",
    "    best_val = np.inf\n",
    "    \n",
    "    loss_train = np.array([])\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_avg = 0\n",
    "        loss_avg_val = 0\n",
    "        n_loss = 0\n",
    "        n_loss_val = 0\n",
    "        model.train()\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        iterator = tqdm(train_loader)\n",
    "        for batch_x1, batch_x2, batch_y in (iterator):\n",
    "            \n",
    "            batch_x1 = batch_x1.to(device)\n",
    "            batch_x2 = batch_x2.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            batch_x1 = batch_x1.reshape(1, *batch_x1.shape).permute(1,0,2,3)\n",
    "            batch_x2 = batch_x2.reshape(1, *batch_x2.shape).permute(1,0,2,3)\n",
    "            \n",
    "            y_pred = 0\n",
    "                \n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                batch_x_j1 = batch_x1[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                batch_x_j2 = batch_x2[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]\n",
    "                \n",
    "                y_pred_j = model(batch_x_j1, batch_x_j2)\n",
    "        \n",
    "                loss = loss_fn(y_pred_j,batch_y)\n",
    "\n",
    "                n_loss = n_loss+1\n",
    "                loss_avg = loss_avg + loss.detach().cpu().numpy()\n",
    "        \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                iterator.set_description(f\"Train loss: {loss.detach().cpu().numpy()}\")\n",
    "                \n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true = []\n",
    "            for batch_x1, batch_x2, batch_y in (tqdm(valid_loader)):\n",
    "                \n",
    "                batch_x1 = batch_x1.to(device)\n",
    "                batch_x2 = batch_x2.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "            \n",
    "                batch_x1 = batch_x1.reshape(1, *batch_x1.shape).permute(1,0,2,3)\n",
    "                batch_x2 = batch_x2.reshape(1, *batch_x2.shape).permute(1,0,2,3)\n",
    "                \n",
    "                y_pred = 0\n",
    "                \n",
    "                for j in range(max([num_divisions*2-3,1])):\n",
    "                    batch_x_j1 = batch_x1[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                    batch_x_j2 = batch_x2[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]\n",
    "                    \n",
    "                    y_pred_j = model(batch_x_j1, batch_x_j2)\n",
    "                    y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "\n",
    "                    loss_avg_val = loss_avg_val + loss_fn(y_pred_j, batch_y).detach().cpu().numpy()\n",
    "                    n_loss_val = n_loss_val + 1\n",
    "    \n",
    "                predictions.append(y_pred)\n",
    "                true.append(batch_y)\n",
    "                    \n",
    "            predictions = torch.cat(predictions, axis=0)\n",
    "            true = torch.cat(true, axis=0)\n",
    "            val_loss = loss_fn(predictions,true)\n",
    "    \n",
    "            index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "            zeros = np.zeros(predictions.shape)\n",
    "            zeros[np.arange(index.shape[0]), index]=1\n",
    "            \n",
    "            val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "                \n",
    "            print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
    "\n",
    "        loss_val_epoch.append(loss_avg_val/n_loss_val)\n",
    "        loss_train_epoch.append(loss_avg/n_loss)\n",
    "        \n",
    "        if val_loss < best_val:\n",
    "            print(\"Saved Model\")\n",
    "            torch.save(model.state_dict(), model_saving_name)\n",
    "            best_val = val_loss\n",
    "    plt.plot(np.arange(1,epoch+2),loss_train_epoch,label='Training Loss')\n",
    "    plt.plot(np.arange(1,epoch+2),loss_val_epoch,label='Validation Loss (\"segment-wise\")')\n",
    "    plt.legend()\n",
    "\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522724e-da80-4891-b1ad-59e6e079eae3",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54350ab8-5fec-4261-9669-69f14d551ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model.pt' #Name of the model we want to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e341f0a-8897-4c60-8aed-f782fcd3c6bc",
   "metadata": {},
   "source": [
    "#### 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34b580-5f7b-4979-8e86-47ea00c725c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_dim == '1d':\n",
    "    model = Network1d(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                                                                                 \n",
    "    test_data = Dataset_1d(test, labels_onehot)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true = []\n",
    "        for batch_x, batch_y in tqdm(test_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "            \n",
    "\n",
    "            y_pred = 0\n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                \n",
    "                batch_x_j = batch_x[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                \n",
    "                y_pred_j = model(batch_x_j)\n",
    "                y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "            \n",
    "            predictions.append(y_pred)\n",
    "            true.append(batch_y)\n",
    "        predictions = torch.cat(predictions, axis=0)\n",
    "        true = torch.cat(true, axis=0)\n",
    "        val_loss = loss_fn(predictions, true)\n",
    "\n",
    "        index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "        zeros = np.zeros(predictions.shape)\n",
    "        zeros[np.arange(index.shape[0]), index]=1\n",
    "        \n",
    "        val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "            \n",
    "        print(f\"loss: {val_loss}, accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d12216f-dfc9-46bd-a18b-35de90f00309",
   "metadata": {},
   "source": [
    "#### 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0170dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_dim == '2d':\n",
    "    model = Network2d(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                                                                                 \n",
    "    test_data = Dataset_2d(test, labels_onehot)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true = []\n",
    "        for batch_x, batch_y in tqdm(test_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "            \n",
    "\n",
    "            y_pred = 0\n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                \n",
    "                batch_x_j = batch_x[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                \n",
    "                y_pred_j = model(batch_x_j)\n",
    "                y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "            \n",
    "            predictions.append(y_pred)\n",
    "            true.append(batch_y)\n",
    "        predictions = torch.cat(predictions, axis=0)\n",
    "        true = torch.cat(true, axis=0)\n",
    "        val_loss = loss_fn(predictions, true)\n",
    "\n",
    "        index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "        zeros = np.zeros(predictions.shape)\n",
    "        zeros[np.arange(index.shape[0]), index]=1\n",
    "        \n",
    "        val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "            \n",
    "        print(f\"loss: {val_loss}, accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b5a32-7de0-439c-979f-e0065a96416c",
   "metadata": {},
   "source": [
    "#### Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_dim == 'both':\n",
    "    model = NetworkBoth(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                                                                                 \n",
    "    test_data = Dataset_both(test, labels_onehot)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true = []\n",
    "        for batch_x1, batch_x2, batch_y in (tqdm(test_loader)):\n",
    "            \n",
    "            batch_x1 = batch_x1.to(device)\n",
    "            batch_x2 = batch_x2.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "        \n",
    "            batch_x1 = batch_x1.reshape(1, *batch_x1.shape).permute(1,0,2,3)\n",
    "            batch_x2 = batch_x2.reshape(1, *batch_x2.shape).permute(1,0,2,3)\n",
    "            \n",
    "            y_pred = 0\n",
    "            \n",
    "            for j in range(num_divisions*2-3):\n",
    "                batch_x_j1 = batch_x1[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                batch_x_j2 = batch_x2[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]\n",
    "                \n",
    "                y_pred_j = model(batch_x_j1, batch_x_j2)\n",
    "                y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "\n",
    "            predictions.append(y_pred)\n",
    "            true.append(batch_y)\n",
    "                \n",
    "        predictions = torch.cat(predictions, axis=0)\n",
    "        true = torch.cat(true, axis=0)\n",
    "        val_loss = loss_fn(predictions, true)\n",
    "\n",
    "        index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "        zeros = np.zeros(predictions.shape)\n",
    "        zeros[np.arange(index.shape[0]), index]=1\n",
    "        \n",
    "        val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "            \n",
    "        print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
    "else:\n",
    "    None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2e142-ad51-459b-b5d3-832e118310d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
